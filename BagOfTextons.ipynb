{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from filter import *\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.cluster import KMeans,MiniBatchKMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from libKMCUDA import kmeans_cuda\n",
    "from scipy.cluster.vq import vq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from disk to memory\n",
    "def readData(path): \n",
    "    all_food_data = pd.read_csv(path, sep=\"\\t\", names=['imageName', 'dishName'])\n",
    "\n",
    "    all_pic_names = all_food_data.drop('dishName', axis=1)\n",
    "    all_dish_name = all_food_data['dishName']\n",
    "\n",
    "\n",
    "    return all_pic_names, all_dish_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from disk to memory\n",
    "def readData2(path): \n",
    "    all_food_data = pd.read_csv(path, sep=\"\\t\", names=['imageName', 'dishName'])\n",
    "\n",
    "    x = all_food_data.drop('dishName', axis=1)\n",
    "    y = all_food_data['dishName']\n",
    "\n",
    "    #Dropped all multilabel data\n",
    "    y=y.str.split(\" \", n = 1, expand = True)\n",
    "    data =  pd.concat([x, y], axis=1, join='inner')\n",
    "    data.columns = ['imageName', 'dishName', 'trivial']\n",
    "    data = data[pd.isnull(data['trivial'])]\n",
    "    data = data.drop('trivial', axis=1)\n",
    "    data = data.reset_index()\n",
    "    del data['index']\n",
    "    \n",
    "    all_pic_names = data.drop('dishName', axis=1)\n",
    "    all_dish_name = data['dishName']\n",
    "\n",
    "    \n",
    "    return all_pic_names, all_dish_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractSchmidResponses(img_paths):\n",
    "    schmid_filters = make_schmid_filters()\n",
    "    responses = np.array([])\n",
    "    base_path = '../Data/UNICT-FD1200_Small/'\n",
    "    \n",
    "    full_path = base_path + img_paths.iloc[0]['imageName']\n",
    "    print(full_path)\n",
    "    img = cv2.imread(full_path)\n",
    "    lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "    responses =  [apply_filter_bank(lab, schmid_filters)]\n",
    "    \n",
    "    for path in img_paths.iloc[1:]['imageName']:\n",
    "        full_path = base_path + path\n",
    "        img = cv2.imread(full_path)\n",
    "        lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "        tmp = apply_filter_bank(lab, schmid_filters)\n",
    "        responses = np.append(responses, [tmp] , axis=0)\n",
    "        print(path)\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_histogram(descriptor_list, cluster_alg):\n",
    "    histogram = np.zeros(len(cluster_alg.cluster_centers_))\n",
    "    cluster_result =  cluster_alg.predict(descriptor_list)\n",
    "    for i in cluster_result:\n",
    "        histogram[i] += 1.0\n",
    "    return histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_path, y_train = readData(\"Train_1.txt\")\n",
    "x_test_path, y_test = readData2(\"Test_1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savedResponsesFiles = os.listdir('files')\n",
    "if 'train_responses12000Lab.npy' not in savedResponsesFiles:\n",
    "    print('Train response features not found!')\n",
    "    train_response_vector = extractSchmidResponses(x_train_path)\n",
    "    np.save('files/train_responses12000Lab', train_response_vector)\n",
    "    print('Train response features created!')\n",
    "else:\n",
    "    print('Train response fetaures found!')\n",
    "    train_response_vector = np.load('files/train_responses12000Lab.npy')\n",
    "    print('Train response features loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_response_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_response_vector = train_response_vector.reshape(train_response_vector.shape[0]*train_response_vector.shape[1], 21) # convert from (1200, 76800)\n",
    "train_response_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterResponses(responses):    \n",
    "    #kmeans = KMeans(init='k-means++', n_clusters=1200, verbose=2, a)\n",
    "    #kmeans = MiniBatchKMeans(init='k-means++', n_clusters=1200, batch_size=360000 ,verbose=2)\n",
    "    responses = np.split(responses, 8)\n",
    "    #print(str((responses[0].nbytes/1024)/1024))\n",
    "    centroids, _ = kmeans_cuda(responses[0], 12002, init=\"k-means++\",verbosity=2, yinyang_t=0)\n",
    "    \n",
    "    for arr in range(1,8):\n",
    "        centroids, assignments = kmeans_cuda(arr, 12002, init=centroids, verbosity=2, yinyang_t=0)\n",
    "    #kmeans.fit(responses)    \n",
    "    #vocabulary = [kmeans.cluster_centers_]\n",
    "    \n",
    "    return centroids, assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#centroids, assigments = clusterResponses(train_response_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = np.load('files/centroids.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(np.isnan(centroids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(init=centroids, n_clusters=12000, verbose=2, max_iter=1)\n",
    "kmeans.fit(train_response_vector[:12000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = np.array(np.delete(centroids, 407, 0))\n",
    "centroids = np.array(np.delete(centroids, 6656, 0))\n",
    "np.any(np.isnan(centroids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('vocabulary200NoMultiLabel', vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = np.array(vocabulary)\n",
    "vocabulary = vocabulary.reshape(vocabulary.shape[0]*vocabulary.shape[1], 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = extractSchmidResponses(x_test_path[1511:1512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array(test).reshape(76800, 21)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = np.array(test)\n",
    "#test = test.reshape(76800,21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = build_histogram(test, kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_image = []\n",
    "for i in range(200):\n",
    "    img = x_train_path.iloc[i:i+1]\n",
    "    resp = extractSchmidResponses(img)\n",
    "    resp = np.array(resp)\n",
    "    resp = resp.reshape(resp.shape[0]*resp.shape[1],21)\n",
    "    if (resp is not None):\n",
    "        histogram = build_histogram(resp, model)\n",
    "        preprocessed_image.append(histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_test_image = []\n",
    "for i in range(200):\n",
    "    img = x_test_path.iloc[i:i+1]\n",
    "    resp = extractSchmidResponses(img)\n",
    "    resp = np.array(resp)\n",
    "    resp = resp.reshape(resp.shape[0]*resp.shape[1],21)\n",
    "    if (resp is not None):\n",
    "        histogram = build_histogram(resp, model)\n",
    "        preprocessed_test_image.append(histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('preprocessed_train_image_NL_200',preprocessed_image)\n",
    "np.save('preprocessed_test_image_NL_200',preprocessed_test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.load('preprocessed_train_image_NL_200.npy')#np.array(preprocessed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1, metric='euclidean')\n",
    "knn.fit(a, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.load('preprocessed_test_image_NL_200.npy')#preprocessed_test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = knn.predict(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(pr, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "print(classification_report(pr, y_test))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_ml import ConfusionMatrix\n",
    "confusion_matrix = ConfusionMatrix(y_test, pr )\n",
    "confusion_matrix.plt()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test, pr, classes=y_test,\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_response_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_response_vector = extractSchmidResponses(x_train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('files/train_responses1200', train_response_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 100, 0.001) # I don't know what the parameters mean\n",
    "BoW = cv2.BOWKMeansTrainer(1200, tc,1,cv2.KMEANS_PP_CENTERS)# create a BagOfFeaturesWord. It can be take a long time \n",
    "vocabulary = BoW.cluster(train_response_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_responses = extractSchmidResponses(x_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('files/train_responses.csv', all_train_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = clusterResponses(all_train_responses)kme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.save('files/train_clusters.csv', all_train_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_centroids_histogram(voc, model):\n",
    "\n",
    "    feature_vectors=[]\n",
    "    class_vectors=[]\n",
    "    \n",
    "    for item in voc:\n",
    "        predict_kmeans = model.predict(item)\n",
    "        hist, bin_edges = np.histogram(predict_kmeans, 1200)\n",
    "        feature_vectors.append(hist)\n",
    "        \n",
    "    feature_vectors = np.asarray(feature_vectors)\n",
    "    \n",
    "    return feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=1200).fit(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(words, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = calculate_centroids_histogram(X_train, kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = calculate_centroids_histogram(X_test, kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(preprocessed_image, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  \n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(train)\n",
    "\n",
    "train = scaler.transform(train)  \n",
    "test = scaler.transform(test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "classifier = KNeighborsClassifier(n_neighbors=10)  \n",
    "classifier.fit(train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "print(confusion_matrix(y_test, y_pred))  \n",
    "print(classification_report(y_test, y_pred))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.svm import SVC  \n",
    "#svclassifier = SVC(kernel='poly', degree=8)  \n",
    "#svclassifier.fit(preprocessed_image, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svclassifier.predict(b)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "print(confusion_matrix(y_test, y_pred))  \n",
    "print(classification_report(y_test, y_pred))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
